{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions to Linear Regression\n",
    "\n",
    "Last week we introduced the basics of how to do simple linear regression in Python. However, that's obviously not the standard way we do analysis in economics. We need to dig a little deeper.\n",
    "\n",
    "First, we'll start off by adding more variables to our regression, including how to use dummy variables. \n",
    "\n",
    "Then, we'll look at endogeneity and how to implement IV regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the same packages we used last week:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import statsmodels.api as sm # This is where we will get OLS\n",
    "from statsmodels.iolib.summary2 import summary_col # This is a handy way to look at model results side-by-side\n",
    "\n",
    "# For IV, we will need to pull from another package that is not included in Anaconda, so we need to install it:\n",
    "# !pip install linearmodels\n",
    "from linearmodels.iv import IV2SLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Regression\n",
    "\n",
    "We'll continue with the same data we were looking at in last week's notes. If you don't know what I'm talking about look at those. The authors of that paper had some extended models in their Table 2, so we can try to replicate those. Therefore, we'll need the Table 2 data. It's on [GitHub](https://github.com/UC-Davis-ARE-Econ/Python_Boot_Camp/blob/master/Data/maketable2.dta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from that dumb Stata .dta file:\n",
    "df2 = pd.read_stata('https://github.com/UC-Davis-ARE-Econ/Python_Boot_Camp/raw/master/Data/maketable2.dta')\n",
    "\n",
    "# Add a constant term (See Week 3 notes if you don't know why):\n",
    "df2['constant'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, we have to put together the collections of $X$ variables we want to use as our explanatory variables. As we talked about last week, you have to be rather specific with Python and tell it which variables you want to use at any given time. So when you run a regression using the `OLS` function from `statsmodels` you will specify the variables to use.\n",
    "\n",
    "There are two ways to do this. First, we can create a new object for each of the collections of $X$ variables. That is, we can create a new dataframe that is a subset of the original dataframe for each set of variables we want to use. We do that by indexing:\n",
    "\n",
    "    X1 = df2['constant', 'avexpr']\n",
    "    X2 = df2['constant', 'avexpr', 'lat_abst']\n",
    "    X3 = df2['constant', 'avexpr', 'lat_abst', 'asia', 'africa', 'other']\n",
    "   \n",
    "Pretty simple, and not dissimilar from things we have done before. However, each of these new dataframes is created in memory, so if you are dealing with a big dataset you may have just eaten up a lot of your available RAM. That's because we are essentially copying the data that are held in memory already in `df2`. \n",
    "\n",
    "There's another way to do this that avoids creating copies of the dataframe. In this case we will just create lists that contain the indexes we want to call. Then, we can use the list as the index for `df2` when we actually need to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = ['constant', 'avexpr']\n",
    "X2 = ['constant', 'avexpr', 'lat_abst']\n",
    "X3 = ['constant', 'avexpr', 'lat_abst', 'asia', 'africa', 'other']\n",
    "y = ['logpgp95']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can fit our three models using the `OLS` function and the `.fit()` method. Note that I am also making sure to specify that we want robust standard errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg1 = sm.OLS(df2['logpgp95'], df2[X1], missing = 'drop').fit(cov_type = 'HC0')\n",
    "reg2 = sm.OLS(df2['logpgp95'], df2[X2], missing = 'drop').fit(cov_type = 'HC0')\n",
    "reg3 = sm.OLS(df2['logpgp95'], df2[X3], missing = 'drop').fit(cov_type = 'HC0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, Python doesn't show us these results automatically. We could use the `.summary()` method on the fitted regression objects like we did last week, but that would also only show us results one at a time.\n",
    "\n",
    "Instead, we can use another function from `statsmodels` called `summary_col`, which will show us the results of the three models side-by-side. There are a couple things we need to do to set this up, but it should be a good way to view the results.\n",
    "\n",
    "First, we need to use a dictionary to indicate which additional data points we want to include. Let's say we also want to see $R^2$ and the number of observations. We create a dictionary object that will pull these data points from the regression objects when `summary_col` needs them. Note the use of the anonymous functions with the `lambda` in the code. This is so that we do not have to write a new function just to pull these values, and Python will do it automatically when reading the dictionary. \n",
    "\n",
    "The `summary_col` function also gives us several options to choose from. `float_format` will let us choose the number of decimal places to round to, `model_names` lets us give names to each column, and `regressor_order` allows us to specify where to display the coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Table 2 - OLS Regressions\n",
      "=========================================\n",
      "                 Model 1 Model 2 Model 3 \n",
      "-----------------------------------------\n",
      "constant         4.63*** 4.87*** 5.85*** \n",
      "                 (0.24)  (0.28)  (0.29)  \n",
      "avexpr           0.53*** 0.46*** 0.39*** \n",
      "                 (0.03)  (0.05)  (0.05)  \n",
      "lat_abst                 0.87*   0.33    \n",
      "                         (0.49)  (0.43)  \n",
      "asia                             -0.15   \n",
      "                                 (0.18)  \n",
      "africa                           -0.92***\n",
      "                                 (0.15)  \n",
      "other                            0.30*   \n",
      "                                 (0.17)  \n",
      "R-squared        0.61    0.62    0.72    \n",
      "No. observations 111     111     111     \n",
      "=========================================\n",
      "Standard errors in parentheses.\n",
      "* p<.1, ** p<.05, ***p<.01\n"
     ]
    }
   ],
   "source": [
    "# Set up dictionary to specify additional values to include:\n",
    "info_dict = {'R-squared' : lambda x: f\"{x.rsquared:.2f}\",\n",
    "           'No. observations' : lambda x: f\"{int(x.nobs):d}\"}\n",
    "\n",
    "# Construct the table object with the options we want:\n",
    "results_table = summary_col(results=[reg1,reg2,reg3],\n",
    "                            float_format='%0.2f',\n",
    "                            stars = True,\n",
    "                            model_names=['Model 1',\n",
    "                                         'Model 2',\n",
    "                                         'Model 3'],\n",
    "                            info_dict=info_dict,\n",
    "                            regressor_order=['constant',\n",
    "                                             'avexpr',\n",
    "                                             'lat_abst',\n",
    "                                             'asia',\n",
    "                                             'africa'])\n",
    "\n",
    "# Add a title so it looks nice:\n",
    "results_table.add_title('Table 2 - OLS Regressions')\n",
    "\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, it even has stars for significant coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Variables\n",
    "\n",
    "We've actually already run a regression with a couple of dummy variables. Note that `asia`, `africa`, and `other` are dummy variables, simply indicating where the country is located. So, we can see that there is no need to treat them any differently than other variables when working with these regression functions.\n",
    "\n",
    "There is one more thing worth noting about dummy variables. The `statsmodels` package actually includes are very useful function for creating dummy variables from a single category variable. Suppose that instead of having several dummy variables already, we just have one variable with values of `asia`, `africa`, and `other`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>africa</th>\n",
       "      <th>asia</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>africa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>other</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>asia</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asia</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>africa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>africa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>africa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>africa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>other</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>asia</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location africa asia other\n",
       "0   africa    1.0  0.0   0.0\n",
       "1    other    0.0  0.0   1.0\n",
       "2     asia    0.0  1.0   0.0\n",
       "3     asia    0.0  1.0   0.0\n",
       "4   africa    1.0  0.0   0.0\n",
       "5   africa    1.0  0.0   0.0\n",
       "6   africa    1.0  0.0   0.0\n",
       "7   africa    1.0  0.0   0.0\n",
       "8    other    0.0  0.0   1.0\n",
       "9     asia    0.0  1.0   0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create some fake data:\n",
    "np.random.seed(582)\n",
    "fake_category = np.random.choice(['asia', 'africa', 'other'], 10)\n",
    "\n",
    "# Generate dummy variables from this list: \n",
    "dummies = sm.categorical(fake_category)\n",
    "\n",
    "pd.DataFrame(dummies, columns = ['location', 'africa', 'asia', 'other'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endogeneity\n",
    "\n",
    "We know that there are lots of cases where endogeneity is an issue. In fact, this paper we are replicating specifically refers to an endogenous relationship between the economic outcome, GDP, and the strength of institutions. It's definitely a case of chicken-and-egg, so we should be worried about the relationship between `logpgp95` and `avexpr`.\n",
    "\n",
    "We'll explore how to do this two ways: first we'll go through the two stages of two-stage least squares, then we'll use the command included in `statsmodels`. \n",
    "\n",
    "The paper we are replicating uses mortality rates as an instrument for institutional differences in the protection against appropriation. I don't know how realistic that it, but they do demonstrate that there is a relationship between these two variables, so there it is at least a valid instrument. \n",
    "\n",
    "Let's jump in to the first stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with the instrument:\n",
    "df3 = pd.read_stata('https://github.com/UC-Davis-ARE-Econ/Python_Boot_Camp/raw/master/Data/maketable4.dta')\n",
    "df3 = df3[df3['baseco'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to talk about that last line for a minute. Notice that we are subsetting the data to only `baseco == 1`. This is because apparently `baseco` indicates which entries have no missing data. That's good to know.\n",
    "\n",
    "But notice also that we are essentially using to indices here. First, we are indexing the dataframe, and then we are using an index within that index to refer to a specific variable. We are essentially saying only keep the rows for which `baseco` is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 avexpr   R-squared:                       0.270\n",
      "Model:                            OLS   Adj. R-squared:                  0.258\n",
      "Method:                 Least Squares   F-statistic:                     16.85\n",
      "Date:                Thu, 16 Jan 2020   Prob (F-statistic):           0.000120\n",
      "Time:                        18:28:15   Log-Likelihood:                -104.83\n",
      "No. Observations:                  64   AIC:                             213.7\n",
      "Df Residuals:                      62   BIC:                             218.0\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:                  HC0                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "constant       9.3414      0.693     13.476      0.000       7.983      10.700\n",
      "logem4        -0.6068      0.148     -4.105      0.000      -0.897      -0.317\n",
      "==============================================================================\n",
      "Omnibus:                        0.035   Durbin-Watson:                   2.003\n",
      "Prob(Omnibus):                  0.983   Jarque-Bera (JB):                0.172\n",
      "Skew:                           0.045   Prob(JB):                        0.918\n",
      "Kurtosis:                       2.763   Cond. No.                         19.4\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC0)\n"
     ]
    }
   ],
   "source": [
    "# Add a constant:\n",
    "df3['constant'] = 1\n",
    "\n",
    "# Fit the first-stage regression and display the results:\n",
    "first_stage_results = sm.OLS(df3['avexpr'], df3[['constant', 'logem4']], missing = 'drop').fit(cov_type = 'HC0')\n",
    "print(first_stage_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate the second stage of the model we need the predicted values of the first-stage model. Unsurprisingly, there is a `.predict()` method that we can use. We'll just store the predicted values as a new column in our dataframe, then we can run the second-stage regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               logpgp95   R-squared:                       0.477\n",
      "Model:                            OLS   Adj. R-squared:                  0.469\n",
      "Method:                 Least Squares   F-statistic:                     63.65\n",
      "Date:                Thu, 16 Jan 2020   Prob (F-statistic):           4.32e-11\n",
      "Time:                        18:32:22   Log-Likelihood:                -72.268\n",
      "No. Observations:                  64   AIC:                             148.5\n",
      "Df Residuals:                      62   BIC:                             152.9\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:                  HC0                                         \n",
      "====================================================================================\n",
      "                       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------\n",
      "constant             1.9097      0.745      2.563      0.010       0.449       3.370\n",
      "predicted_avexpr     0.9443      0.118      7.978      0.000       0.712       1.176\n",
      "==============================================================================\n",
      "Omnibus:                       10.547   Durbin-Watson:                   2.137\n",
      "Prob(Omnibus):                  0.005   Jarque-Bera (JB):               11.010\n",
      "Skew:                          -0.790   Prob(JB):                      0.00407\n",
      "Kurtosis:                       4.277   Cond. No.                         58.1\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC0)\n"
     ]
    }
   ],
   "source": [
    "# Create a new column for the predicted values:\n",
    "df3['predicted_avexpr'] = first_stage_results.predict()\n",
    "\n",
    "# Estimate the second-stage regression and display results:\n",
    "second_stage_results = sm.OLS(df3['logpgp95'], df3[['constant', 'predicted_avexpr']]).fit(cov_type = 'HC0')\n",
    "print(second_stage_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How's this compare to the original regression? Let's use `summary_col` again to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "                 Old and Busted New Hotness\n",
      "-------------------------------------------\n",
      "constant         4.63***        1.91**     \n",
      "                 (0.24)         (0.75)     \n",
      "avexpr           0.53***                   \n",
      "                 (0.03)                    \n",
      "predicted_avexpr                0.94***    \n",
      "                                (0.12)     \n",
      "R-squared        0.61           0.48       \n",
      "No. observations 111            64         \n",
      "===========================================\n",
      "Standard errors in parentheses.\n",
      "* p<.1, ** p<.05, ***p<.01\n"
     ]
    }
   ],
   "source": [
    "iv_results_table = summary_col(results = [reg1, second_stage_results],\n",
    "                               float_format = '%0.2f',\n",
    "                               stars = True,\n",
    "                               model_names = ['Old and Busted', 'New Hotness'],\n",
    "                               info_dict = info_dict,\n",
    "                               regressor_order = ['constant', 'avexpr', 'predicted_avexpr'])\n",
    "print(iv_results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty major change in the coefficient, so it's a good thing we accounted for the endogeneity. Of course, from the number of times we've done this sort of two-step estimation procedure we know that the standard errors will not be correctly calculated for the second-stage model.\n",
    "\n",
    "We can, of course, correct for that by using the proper function. In `linearmodels` this function is `IV2SLS`, again a pretty straightforward name. Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          IV-2SLS Estimation Summary                          \n",
      "==============================================================================\n",
      "Dep. Variable:               logpgp95   R-squared:                      0.1870\n",
      "Estimator:                    IV-2SLS   Adj. R-squared:                 0.1739\n",
      "No. Observations:                  64   F-statistic:                    28.754\n",
      "Date:                Thu, Jan 16 2020   P-value (F-stat)                0.0000\n",
      "Time:                        19:39:16   Distribution:                  chi2(1)\n",
      "Cov. Estimator:                robust                                         \n",
      "                                                                              \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "constant       1.9097     1.1740     1.6267     0.1038     -0.3912      4.2106\n",
      "avexpr         0.9443     0.1761     5.3623     0.0000      0.5991      1.2894\n",
      "==============================================================================\n",
      "\n",
      "Endogenous: avexpr\n",
      "Instruments: logem4\n",
      "Robust Covariance (Heteroskedastic)\n",
      "Debiased: False\n"
     ]
    }
   ],
   "source": [
    "iv_reg = IV2SLS(dependent = df3['logpgp95'],\n",
    "                exog = df3['constant'],\n",
    "                endog = df3['avexpr'],\n",
    "                instruments = df3['logem4']).fit()\n",
    "print(iv_reg.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two things to note here that are different from the `OLS` function. First, while the arguments are named in a very clear way again, as Olivia pointed out there is a difference between \"dependent\" and \"endogenous,\" especially when considering IV. There are separate arguments for each here, so you have to be clear about what variable is your problem child, and then which instruments you are using for it.\n",
    "\n",
    "Second, the default covariance estimator is already robust to heteroskedasticity. That's convenient, now we don't need to correct for that. I do not know which specific covariance estimator is used, but that's probably fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summing Up\n",
    "\n",
    "So now we know how to do linear regression, both with and without IV. That's pretty useful! We should all be able to carry out at least the most basic analysis at this point.\n",
    "\n",
    "The two exercises in the text book are actually great for this chapter as well. I'd recommend trying them out. They ask you to do a Hausman test to check the endogeneity of the `avexpr` variable and to do OLS through matrix algebra. Both are useful ways to check whether you know how to run regressions and how to work with arrays. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
